= Resource Efficiency in Replication
:revdate: 2024-02-17
:page-hidden: true
:stem: latexmath
:page-features: stem, plot
:toc: preamble
:nospace:

A distributed database is formed of a number of individual machines.  The aggregate storage capacity, and read and write throughput, of the distributed database is less than or equal to the sum of what each individual machine can offer in terms of its throughput or storage capacity.  This ratio of input resources versus resulting cluster throughput is our Resource Efficiency, and is greatly affected by the choice of replication algorithm.

== Introduction

When deploying a distributed database, one forms a cluster of machines stem:[X], each of which, if individually benchmarked as a one-machine cluster, can deliver a read throughput stem:[R_X], write throughput stem:[W_X], and has available storage capacity stem:[S_X].  The resulting aggregate, overall database system stem:[Y] can be benchmarked to determine its read throughput stem:[R_Y], write throughput stem:[W_Y] and storage capacity stem:[S_Y].  Throughout this post, we'll be examining what each class of replication algorithms can deliver in terms of the maximum values for read throughput efficiency stem:[R_Y / R_X], write throughput efficiency stem:[W_Y / W_X], and storage capacity efficiency stem:[S_Y / S_X].  As a counterpoint to resource efficiency, we'll also be looking at availability: given the loss of a random machine, what's the chance that a user will observe a transient service disruption?

This post is also one that lives purely in the land of theory.  All implementation efficiency costs are handwaved as negligible, which is blatantly false as evidenced by e.g. Red Panda's claims of being more resource efficient in aggregate than Kafka.  In the "write bandwidth efficiency" above, we're only concerning ourself with the network bandwidth.  Storage efficiency also captures the disk write bandwidth efficiency, as storing 3 copies of data requires writing 3 copies of data (or, likely, far more considering storage engine write amplification).  It's tremendously more likely that in a real production deployment, the bottleneck for writes will be the disk and not the network.  However, we're discussing the theoretical tradeoffs only, because write bandwidth throughput illustrates how replication topology matters for efficiency.

The data replication algorithms examined fall across two categories: those designed to quietly tolerate failing nodes (failure masking), and those necessitating explicit reconfiguration around failures (failure detection).  We'll be examining leaderless replication for failure masking algorithms, and chain replication for failure detection algorithms.  Algorithms also exist between these two extremes, such as leaderful consensus which requires detecting failures only for the leader, and masks failures for the followers.

All of this analysis is building to one core, yet obvious point: failure detection-based algorithms only require stem:[f+1] nodes to tolerate stem:[f] failures, which is less than failure masking's stem:[2f+1] nodes.  Superficially, that's 40% more resources to accomplish the same task (for the common stem:[f=2]).  But how does that compare after we take the algorithm's resource efficiency into account?  Do the benefits of a leadered failure masking protocol like Raft outweigh the detriments, as opposed to leaderless consensus?

== Raft / Multi-Paxos

:uri-cockroach-follower-reads: https://www.cockroachlabs.com/blog/follower-reads-stale-data/
:uri-tikv-follower-reads: https://tikv.org/blog/double-system-read-throughput/
:uri-edb-pgdist-witness: https://www.enterprisedb.com/docs/pgd/latest/node_management/witness_nodes/
:uri-spanner-witness: https://cloud.google.com/spanner/docs/replication#witness
:uri-ydb-erasure-encode: https://ydb.tech/docs/en/concepts/cluster/distributed_storage
:uri-tigerbeetle-fpaxos: https://docs.tigerbeetle.com/deploy/hardware/

Leaderful consensus what is generally brought to mind when one mentions "consensus".  It is best known as <<Raft>>, <<MultiPaxos>> or <<ZAB>>, and exemplified by distributed databases such as <<CockroachDB>>, <<TiDB>> and <<Spanner>>, or configuration management systems such as <<PaxosMadeLive>> and <<Zookeeper>>.  (Among _many_ other high-quality, production systems.)

In the simplest Raft implementation, all operations sent to the leader, and the leader broadcasts the replication stream to its followers.  Tolerating stem:[f=2] failures requires stem:[2f+1 = 5] nodes.  All nodes store and write the same data to disk.  At most two of the replicas are permitted to be unavailable.  Across any and all flavors of Raft and Multi-Paxos, the presence of a leader is fundamental, and gives a 1 in 5 chance of transient unavailability if a node fails.

[graphviz]
----
digraph G {
  Client -> Leader   [dir=both];
  Leader -> Replica1 [dir=both];
  Leader -> Replica2 [dir=both];
  Leader -> Replica3 [dir=both, style=dashed];
  Leader -> Replica4 [dir=both, style=dashed];
}
----

All replicas store the same data, and thus 1/5th of the total storage capacity is available post-replication.  There are a wide set of storage optimizations that have all seen little adoption in industry.  <<WitnessReplicas>> permit removing the majority of the storage from 2/5ths of the replication group, and only {uri-edb-pgdist-witness}[EnterpriseDB Postgres Distributed] and {uri-spanner-witness}[(Cloud) Spanner] implement support for them.  Note though, that removing storage means that witness replicas can't serve reads.  The other possible direction for storage efficiency improvement is <<ErasureEncodedRaft>>.  Erasure encoding is popular in distributed filesystems and blob storage systems, but incredibly rare in distributed databases; I am only aware of {uri-ydb-erasure-encode}[YDB] using it.  Applying <<FlexiblePaxos>> allows one to run with 4 replicas and require 3 to be alive for an election but only replicate across 2.  We're using stem:[f=2] as our comparison baseline, and that's a half way point between stem:[f=1] and stem:[f=2] that doesn't compare equally to anything else, but it is an option which as far as I know only {uri-tigerbeetle-fpaxos}[TigerBeetle] implements.  Thus, as 99% of the Raft implementations one might ever encounter have a storage efficiency of 1/5th, that is the value that will be used for storage efficiency for the rest of the analysis.

Read throughput can be improved by implementing <<LinearizableQuorumReads>> for 2/5ths read throughput, <<PaxosQuorumLeases>> for 3/5ths read throughput, or <<FollowerReads>> for 5/5ths read throughput at the cost of increased latency.  We'll disregard the latency implications, and keep 5/5ths as Raft's read throughput, which is realistic given that it's been implemented in production systems such as {uri-cockroach-follower-reads}[Cockroach] and {uri-tikv-follower-reads}[TiKV].

In classic Raft, all proposals go to the leader, and then the leader broadcasts the proposals to all followers.  This means Raft is first constraining to utilizing only stem:[1/(2f+1)] or 1/5th of the available incoming bandwidth.  Then the bottleneck becomes the leader's outgoing bandwidth, further reduction of stem:[1/2f], so 1/4th.  This means a write bandwidth efficiency of stem:[1/(4f^2 + 2f)] or 1/20th.

There have been ways discussed to scale the write bandwidth.  <<PullBasedConsensus>> presents an argument that a fixed topology is not needed, replicas can fetch from other replicas, and thus even a linear chain replicas could work.  <<ScalingReplication>> shows another view that the work of broadcasting to all replicas can be delegated to other replicas.  <<CommutativeRaft>> presents a different approach, in which clients are allowed to directly send to all replicas, and the leader only arbitrates ordering when there's conflicts.  Of these, only pull-based consensus is implemented in industry, but I'm not aware that even MongoDB itself runs in a linear chain configuration.  (It's mostly about saving WAN costs.)  Thus, 1/4th is the value that will be used for write bandwidth efficiency for the rest of the analysis.

A more resource efficient Raft implementation could combine <<FollowerReads>> (enabling all replicas to provide full read throughput), <<PullBasedConsensus>> (to set up a chain-organized replication stream), and <<WitnessReplicas>> (to store only 3 full copies of data).  However, our maximal resource efficiency comes from adopting <<ErasureEncodedRaft>> instead of <<WitnessReplicas>> for the storage savings.  There's a number of issues being handwaved away (how to apply operations when any one replica doesn't know the data, how to successfully do minority reads, that it falls back to full data replication when a node is lost{nospace}sidenote:ref[]), so this is mostly to show the theoretical maximum in an ideal world and less a claim that it's what _should_ be implemented.
[.aside]#sidenote:def[] The storage needs increasing upon a failure was improved in <<ErasureEncodedHRaft>>, and other deficiencies can likely be similarly explored and improved.#

In summary, our resource efficiency for stem:[f=2] for a minimal Raft implementation, Raft with the set of improvements that one will commonly encounter in industry, and our theoretical and maximally resource efficient Raft is:

[cols="1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Simplest
| 20%
| 20%
| 5%
| 20%

| Common Improvements
| 20%
| 100%
| 5%
| 20%

| Follower+Pull+Witness
| 33%
| 60%
| 20%
| 20%

| Follower+Pull+Erasure
| 33%
| 100%
| 20%
| 20%
|===

== Leaderless Paxos / Majority Quorums

The best known leaderless replication algorithms are the Paxos family of protocols.  They do not need to be consensus though, and majority quorums (with <<ReadRepair>>sidenote:ref[]
[.aside]#sidenote:def[] This post accidentally turned into an interesting adventure into finding the correct citation for a number of common concepts.  This is the first, and by no means the last, citation where the source paper tangentially introduced a now fundamental concept.#
) or <<ABD>> are also in this category.  Leaderless replication is used in industry by systems like <<Megastore>> and <<PaxosStore>>, and <<Cassandra>>.

While leaderless Paxos and majority quorums differ in terms of consistency guarantees, they're very similar in terms of resource efficiency.  Tolerating stem:[f=2] failures requires stem:[2f+1=5] nodes.  All nodes store the same data.  Writes are broadcast to all replicas and require a majority of responses.  Reads are broadcast to all replicas and require a majority of responses.

[graphviz]
----
digraph G {
  Client -> Replica1 [dir=both];
  Client -> Replica2 [dir=both];
  Client -> Replica3 [dir=both];
  Client -> Replica4 [dir=both, style=dashed];
  Client -> Replica5 [dir=both, style=dashed];
}
----

There's a much smaller variety of resource efficiency optimizations for leaderless replication algorithms.

<<WitnessReplicas>> applies again, and allows storing only the log of most recent changes on 2 of the 5 nodes, thus bringing the storage efficiency from 20% to 33%.

<<RSPaxos>> examines applying erasure encoding to Paxos log entries, and concludes that space savings can only be obtained if fault tolerance is sacrificed. However, I believe all of <<ErasureEncodedRaft>> should apply equally to leaderless consensus as well, and so we'll assume erasure encoding is feasible.  This brings the storage efficiency to 33% as well.  Additionally, erasure encoding the log entries brings the write efficiency from 20% to 33%, and reading erasure encoded data also brings the read efficiency from 20% to 33%.

The major advantage of leaderless, quorum-based algorithms is the lack of dependence on a leader.  As opposed to Raft and Multi-Paxos, the chance of unavailability on failure is 0%.  There are no leader leases which must first time out, or any reconfiguration step which needs to be done.

[cols="1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Majority Quorums
| 20%
| 20%
| 20%
| 0%

| Paxos
| 20%
| 20%
| 20%
| 0%

| Erasure Encoded Paxos
| 33%
| 33%
| 33%
| 0%
|===

== Reconfigurable Primary-Backup

:uri-apache-pegasus: https://pegasus.apache.org/
:uri-hibari: https://github.com/hibari/hibari
:uri-dan-luu-limplock: https://danluu.com/limplock/

Lastly, there is the lesser known class of failure detection-based replication algorithms.  These are algorithms in which there is a set of nodes in a replication group, and on detected failure, these algorithms execute a _view change_ to reconfigure to a new set of nodes with no failures.  There's a broad examination of such distributed protocols stemming from <<VirtualSynchrony>>, and this perspective of view changes on detected failures is even present in consensus protocols such as <<ViewstampedReplication>>. However, our dividing line for this analysis is that the failure-detection algorithms use stem:[f+1] nodes to tolerate stem:[f] failures.  If stem:[2f+1] nodes are required, then the algorithm is likely better covered by one of the two failure-masking classes of algorithms discussed above.  Viewstamped Replication itself is very much like Raft and Multi-Paxos, and covered by Leadered Consensus above.

As a consequence of only having stem:[f+1] nodes for stem:[f] failures, there is a consistent theme in that all algorithms examined are _not consensus_.  This also means that they cannot solve consensus problems, such as deciding which replicas are responsible for a shard of data, or which node is the primary.  They all rely on an external consensus service to help with those issues.  Think of this as a control plane / data plane split: there's one instance of a consensus service in the control plane orchestrating the small amount of metadata deciding which nodes are in which replication groups responsible for which shards of data, and the horizontally scaleable data plane replicates each shard of data within its assigned group.

There's two shapes of algorithms in this class of failure detection replication protocols: those that look like some form of primary-backup replication where a leader fans out requests to one or more backup nodes, or chain replication-like algorithms where each node is responsible for forwarding each piece of replicated data to the next node in the chain.  In academia, evolving Paxos into a reconfigurable primary-backup replication was examined in <<VerticalPaxosII>>. <<PacificA>> and <<Hermes>> are more recent but different views on reconfigurable primary-backup replication. <<CRAQ>> is the most famous chain replication algorithm, with <<HyperDex>> being a more recently proposed chain-based system.  In industry, <<Kafka>> and <<FoundationDB>>sidenote:ref[][.aside]#sidenote:def[] Disclaimer: former affiliation.# use different variants of reconfigurable primary-backup, {uri-apache-pegasus}[Apache Pegasus] uses PacificA.  Nearly all of the chain replication databases in industry have died out, as {uri-hibari}[hibari] was one of the last but appears abandoned now, and HyperDex almost become a startup.

[cols="1,1"]
|===
^| Chain ^| Parallel

a|
[graphviz]
----
digraph G {
  Client -> Replica1 -> Replica2 -> Replica3 -> Client;
}
----
a|
[graphviz]
----
digraph G {
  Client -> Primary   [dir=both];
  Primary -> Replica1 [dir=both];
  Primary -> Replica2 [dir=both];
}
----
|===

Unlike the quorum systems, there's no opportunity for erasure encoding.  When the number of replicas is stem:[F+1], we expect that a single alive replica can serve reads for all of its data.  Erasure encoding would require multiple pieces, and thus multiple nodes to be available.  Thus, the full cost of 3-way replication is consistently paid, yielding a uniform 33% storage efficiency.

With naive chain replication, only the tail of the chain is allowed to answer read requests, which would give it a read bandwidth efficiency of 33%.  <<CRAQ>> permits any node to answer reads, and thus it gets 100% read bandwidth efficiency.  <<Hermes>> permits any replica to serve reads independently, so it directly gains a 100% read bandwidth efficiency.



Unavailability is the weakpoint of reconfigurable primary-backup systems.  The dependence on all nodes being functioning, and detecting and reconfiguring around failures, means that 

[cols="1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| CRAQ
| 33%
| 100%
| 33%
| 100%

| Hermes
| 33%
| 100%
| 16.5%
| 100%
|===


Lastly, it's important to note that there's a notable complexity of handling gray failures gained by all failure detection-based replication algorithms (the leader in Raft included).  The complexity of Paxos has been widely discussed, and Reconfigurable Primary-Backup has its own source of complexity instead.  By depending on all replicas to be functioning correctly, one needs a very precise definition of what "functioning correctly" means.  If the disk is failing and its throughput drops by 90% or if there's a bad switch causing packet loss and thus TCP throughput drops significantly{nospace}sidenote:ref[], that's not a "correctly functioning" machine, and one would wish to reconfigure around the failure. <<GrayFailureAchillesHeel>> discusses gray failure issues in more detail.  {uri-dan-luu-limplock}[Dan Luu has written about this].
[.aside]#sidenote:def[] I'm pretty sure the #1 cause of times I've been paged awake by a service in the middle of the night has been some networking equipment deciding to drop 1% of packets, and TCP thus slows down to approximately dial-up speeds.  Heartbeats can still be sent, so the service isn't "unavailable", but it sure wasn't working well.#

// TODO: belabor the compelxity a bit more and hunt for more references

== Comparison

Taking the most common selection across each category, we have:

[cols="1,1,1,1,1,1"]
|===
|
| Nodes Required for stem:[F=2]
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Leadered
| 5
| 20%
| 100%
| 6.25%
| 20%

| Leaderless
| 5
| 20%
| 20%
| 20%
| 0%

| Primary-Backup
| 3
| 33%
| 100%
| 16.5%
| 100%
|===

This is using "Common Improvements" from the Leadered section, "Paxos" from the Leaderless section, and "Hermes" from the Reconfigurable Primary-Backup section.  (Using Hermes over CRAQ is a bit of an arbitrary selection, but the write latency of Hermes more closely matches that of Raft, and so it's a bit more of an apples-to-apples comparison.)

Immediately apparent is that Reconfigurable Primary-Backup both requires less nodes, and delivers a uniformly equal-or-better resource efficiency for the less resources used.

Even when examining the most resource optimal variants of each class of replication algorithm (the erasure encoded variants):

[cols="1,1,1,1,1,1"]
|===
|
| Nodes Required for stem:[F=2]
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Follower+Pull+Erasure
| 5
| 33%
| 100%
| 20%
| 20%

| Erasure Encoded Paxos
| 5
| 33%
| 33%
| 33%
| 0%

| CRAQ
| 3
| 33%
| 100%
| 33%
| 100%
|===

We see that erasure encoding just brings each quorum algorithm to the resource efficacy of Reconfigurable Primary-Backup, but still requires 66% more nodes than Reconfigurable Primary-Backup.

Raft blends together Failure Masking and Failure Detection into one replication algorithm, and thus exists as a middle-ground between resource efficiency and high availability.  But this blending leaves Raft in an odd place. By partly being a Failure Detection replication algorithm, a reliable Raft implementation has to pick up all the complexity of detecting gray failures in the same way as a Reconfigurable Primary-Backup algorithm, but without anywhere near the corresponding resource efficiency advantages to justify the complexity.  However, by also being a Failure Masking algorithm, it also must deal with nodes being transiently unavailable, and the corresponding error handling complexity and state space explosion that occurs in tracking that.  

Raft also blends the worst aspects of failure masking (poor read/write throughput efficiency and poor storage efficiency) with the worst aspects of failure detection (transient unavailability on failure) into one replication algorithm that's consistently mediocre.  Looking over the tables, it starts to become hard to justify the complexity in terms of the rewards.  Raft gains over leaderless Paxos a 5x increase in read bandwidth, and accepts a 20% chance of transient unavailability on node loss.  Raft gains over Hermes the 20% chance of transient unavailability, but at the cost of 66% more hardware and worse-or-equal resource efficiency across the board.  It's not feeling like an assuredly good trade.

This isn't to say that Raft is a poor choice of replication algorithm.
Leaderless Paxos struggles with livelock when there's high contention on updating a single item, and Raft doesn't.  There are a number of other resources to consider in a real environment other than what was presented in this post.  CPU, memory, disk IOPS, etc., are all finite resources, which were not discussed, but if those become the limiting factor for performance, then that is the bottleneck and efficiency metric to be mindful of. <<ScaleableButWasteful>> notes that CPU constraints can lead <<MultiPaxos>> to have 2x more throughput than <<EPaxos>>.  If throughput is what determines the amount of hardware you need to buy/rent for your database deployment, and the hardware is CPU constrained, then this is a more impactful efficiency to keep in mind for leaderful vs leaderless quorum replication.  (However, I still claim reconfigurable primary-backup would be even more cost effective!)

When looking at what replication algorithm 

++++
<div id="chart"></div>
++++

.Table of data from which the chart is derived
[%collapsible]
====
[#repldata,cols="1,1,2"]
|===
| System | Replication Algorithm Family | Note

| MongoDB | Leaderful | 
| Redis Cluster | Leaderful | 
| Cassandra | Leaderless | Majority quorum for most.  LWT/Accord is leaderless Consensus
|===
====

++++
<script type="text/javascript">

const df = new dfjs.DataFrame(tableToData('repldata'));
df.show();
const df_count = df
  .rename('Replication Algorithm Family', 'replication')
  .groupBy('replication')
  .aggregate(x => x.count(), 'count');
const df_dbs = df
  .rename('Replication Algorithm Family', 'replication')
  .groupBy('replication')
  .aggregate(x => x.select('System').toArray().join(', '), 'tooltip');
const data = df_count.innerJoin(df_dbs, 'replication').toCollection();

var chart = new G2Plot.Pie('chart', {
  data,
  colorField: 'replication',
  angleField: 'count',
  radius: 0.9,
  label: { type: 'spider', formatter: (datum) => datum.replication },
  legend: false,
  interactions: [{ type: 'element-selected' }, { type: 'element-active' }],
});
chart.render();

</script>
++++

I find it hard to believe that paying for 66% more resources, in exchange for seeing a decrease in p99.9 from better transient unavailability handling is the correct tradeoff for 95% of all systems I looked at.

== Reality

There's probably good reasons we don't see a large number of reconfigurable-primary backup databases:

- Implementing reconfigurable primary-backup requires first having a consensus solution implemented and production ready.
  - Either existing solution like zookeeper/etcd, (existing issues, must become expert, testing hassle)
  - Or implement your own ( a lot of work )
- No great single text for discussing how to implement gray failure handling.

And I suspect the reasons for Raft's dominance are reasonably straightfoward:

- Raft has a single text giving sufficient description of how to implement Raft
  - Evidencied by there being a number of weekend projects implementing raft
- There's many existing implementations of raft
- Consistent mediocrity is consistent, and having a system which isn't bad at anything in particular is quite a good thing.

If you have a raft/multi-paxos implementation already, one could change it into something f+1 rather easily:

1. Implement <<PaxosQuorumLeases>>, so that you nominate 3 of the 5 nodes as required for writes but also able to serve reads independently.
2. Move the logic for leader election to rely on an external consensus group.
3. Reduce the quorum from 3 of 5 nodes to 3 of 3 nodes.

[bibliography]
== References

* [[[Raft]]] https://scholar.google.com/scholar?cluster=12646889551697084617[Diego Ongaro and John Ousterhout. 2014. In search of an understandable consensus algorithm. In Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC'14), USENIX Association, Philadelphia, PA, 305-320.]
* [[[MultiPaxos,Multi-Paxos]]] https://scholar.google.com/scholar?cluster=5393275675498127693[Robbert Van Renesse and Deniz Altinbuken. 2015. Paxos Made Moderately Complex. ACM Comput. Surv. 47, 3 (February 2015). DOI:https://doi.org/10.1145/2673577]
* [[[ZAB]]] https://scholar.google.com/scholar?cluster=13624279146503836178[Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini. 2011. Zab: High-performance broadcast for primary-backup systems. In Proceedings of the 2011 IEEE/IFIP 41st International Conference on Dependable Systems&Networks (DSN '11), IEEE Computer Society, USA, 245-256. DOI:https://doi.org/10.1109/DSN.2011.5958223]
* [[[CockroachDB]]] https://scholar.google.com/scholar?cluster=13649983341597312439[Rebecca Taft, Irfan Sharif, Andrei Matei, Nathan VanBenschoten, Jordan Lewis, Tobias Grieger, Kai Niemi, Andy Woods, Anne Birzin, Raphael Poss, Paul Bardea, Amruta Ranade, Ben Darnell, Bram Gruneir, Justin Jaffray, Lucy Zhang, and Peter Mattis. 2020. CockroachDB: The Resilient Geo-Distributed SQL Database. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data (SIGMOD '20), Association for Computing Machinery, Portland, OR, USA, 1493-1509. DOI:https://doi.org/10.1145/3318464.3386134]
* [[[TiDB]]] https://scholar.google.com/scholar?cluster=4024782010863299783[Dongxu Huang, Qi Liu, Qiu Cui, Zhuhe Fang, Xiaoyu Ma, Fei Xu, Li Shen, Liu Tang, Yuxing Zhou, Menglong Huang, Wan Wei, Cong Liu, Jian Zhang, Jianjun Li, Xuelian Wu, Lingyu Song, Ruoxi Sun, Shuaipeng Yu, Lei Zhao, Nicholas Cameron, Liquan Pei, and Xin Tang. 2020. TiDB: a Raft-based HTAP database. Proc. VLDB Endow. 13, 12 (August 2020), 3072-3084. DOI:https://doi.org/10.14778/3415478.3415535]
* [[[Zookeeper]]] https://scholar.google.com/scholar?cluster=16979330189653726967[Patrick Hunt, Mahadev Konar, Flavio P. Junqueira, and Benjamin Reed. 2010. ZooKeeper: Wait-free Coordination for Internet-scale Systems. In 2010 USENIX Annual Technical Conference (USENIX ATC 10), USENIX Association. Retrieved from https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems]
* [[[PaxosMadeLive,Google Chubby]]] https://scholar.google.com/scholar?cluster=17465339664204453932[Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone. 2007. Paxos Made Live - An Engineering Perspective (2006 Invited Talk). In Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing. Retrieved from http://dx.doi.org/10.1145/1281100.1281103]
* [[[Spanner,Google Spanner]]] https://scholar.google.com/scholar?cluster=3523173873845838643[James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Dale Woodford, Yasushi Saito, Christopher Taylor, Michal Szymaniak, and Ruth Wang. 2012. Spanner: Google's Globally-Distributed Database. In OSDI.]
* [[[ReadRepair,Read Repair]]] https://scholar.google.com/scholar?cluster=9927566946845895796[Dahlia Malkhi and Michael K. Reiter. 1998. Secure and scalable replication in Phalanx. In Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281), 51-58. DOI:https://doi.org/10.1109/RELDIS.1998.740474]
* [[[ABD]]] https://scholar.google.com/scholar?cluster=8138971298707379383[Hagit Attiya, Amotz Bar-Noy, and Danny Dolev. 1995. Sharing memory robustly in message-passing systems. J. ACM 42, 1 (January 1995), 124-142. DOI:https://doi.org/10.1145/200836.200869]
* [[[Megastore]]] https://scholar.google.com/scholar?cluster=75122057060478473[Jason Baker, Chris Bond, James C. Corbett, JJ Furman, Andrey Khorlin, James Larson, Jean-Michel Leon, Yawei Li, Alexander Lloyd, and Vadim Yushprakh. 2011. Megastore: Providing Scalable, Highly Available Storage for Interactive Services. In Proceedings of the Conference on Innovative Data system Research (CIDR), 223-234. Retrieved from http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf]
* [[[PaxosStore]]] https://scholar.google.com/scholar?cluster=12164791380407440973[Jianjun Zheng, Qian Lin, Jiatao Xu, Cheng Wei, Chuwei Zeng, Pingan Yang, and Yunfan Zhang. 2017. PaxosStore: high-availability storage made practical in WeChat. Proc. VLDB Endow. 10, 12 (August 2017), 1730-1741. DOI:https://doi.org/10.14778/3137765.3137778]
* [[[Cassandra]]] https://scholar.google.com/scholar?cluster=9829178954647343079[Avinash Lakshman and Prashant Malik. 2010. Cassandra: a decentralized structured storage system. SIGOPS Oper. Syst. Rev. 44, 2 (April 2010), 35-40. DOI:https://doi.org/10.1145/1773912.1773922]
* [[[VirtualSynchrony,Virtual Synchrony]]] https://scholar.google.com/scholar?cluster=2271986924920893419[K. Birman and T. Joseph. 1987. Exploiting virtual synchrony in distributed systems. In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles (SOSP '87), Association for Computing Machinery, Austin, Texas, USA, 123-138. DOI:https://doi.org/10.1145/41457.37515]
* [[[ViewstampedReplication,Viewstamped Replication]]] https://scholar.google.com/scholar?cluster=13000400770252658813[Barbara Liskov and James Cowling. 2012. Viewstamped Replication Revisited. MIT.]
* [[[WitnessReplicas,Witness Replicas]]] https://scholar.google.com/scholar?cluster=9770669944787144857[Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. 1991. Replication in the harp file system. In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles (SOSP '91), Association for Computing Machinery, Pacific Grove, California, USA, 226-238. DOI:https://doi.org/10.1145/121132.121169]
* [[[ErasureEncodedRaft,Erasure Encoded Raft]]] https://scholar.google.com/scholar?cluster=10123939731603884260[Zizhong Wang, Tongliang Li, Haixia Wang, Airan Shao, Yunren Bai, Shangming Cai, Zihan Xu, and Dongsheng Wang. 2020. CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost. In 18th USENIX Conference on File and Storage Technologies (FAST 20), USENIX Association, Santa Clara, CA, 297-308. Retrieved from https://www.usenix.org/conference/fast20/presentation/wang-zizhong]
* [[[ErasureEncodedHRaft,Erasure Encoded HRaft]]] https://scholar.google.com/scholar?cluster=15724086733201598850[Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 1316-1326. DOI:https://doi.org/10.1109/IPDPS53621.2022.00130]
* [[[FlexiblePaxos,Flexible Paxos]]] https://scholar.google.com/scholar?cluster=6509870440808150538[Heidi Howard, Aleksey Charapko, and Richard Mortier. 2021. Fast Flexible Paxos: Relaxing Quorum Intersection for Fast Paxos. In Proceedings of the 22nd International Conference on Distributed Computing and Networking (ICDCN '21), Association for Computing Machinery, Nara, Japan, 186-190. DOI:https://doi.org/10.1145/3427796.3427815]
* [[[LinearizableQuorumReads,Linearizable Quorum Reads]]] https://scholar.google.com/scholar?cluster=10098760952745259234[Aleksey Charapko, Ailidani Ailijiang, and Murat Demirbas. 2019. Linearizable Quorum Reads in Paxos. In 11th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 19), USENIX Association, Renton, WA. Retrieved from https://www.usenix.org/conference/hotstorage19/presentation/charapko]
* [[[PaxosQuorumLeases,Paxos Quorum Leases]]] https://scholar.google.com/scholar?cluster=2618624974148224118[Iulian Moraru, David G. Andersen, and Michael Kaminsky. 2014. Paxos Quorum Leases: Fast Reads Without Sacrificing Writes. In Proceedings of the ACM Symposium on Cloud Computing (SOCC '14), Association for Computing Machinery, Seattle, WA, USA, 1-13. DOI:https://doi.org/10.1145/2670979.2671001]
* [[[PullBasedConsensus,Pull-Based Consensus in MongoDB]]] https://scholar.google.com/scholar?cluster=3477252701158690968[Siyuan Zhou and Shuai Mu. 2021. Fault-Tolerant Replication with Pull-Based Consensus in MongoDB. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), USENIX Association, 687-703. Retrieved from https://www.usenix.org/conference/nsdi21/presentation/zhou]
* [[[ScalingReplication,Scaling Strongly Consistent Replication]]] https://scholar.google.com/scholar?cluster=1909096821088376701[Aleksey Charapko, Ailidani Ailijiang, and Murat Demirbas. 2021. PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus. In Proceedings of the 2021 International Conference on Management of Data (SIGMOD '21), Association for Computing Machinery, Virtual Event, China, 235-247. DOI:https://doi.org/10.1145/3448016.3452834]
* [[[CommutativeRaft,Exploiting Commutativity For Practical Fast Replication]]] https://scholar.google.com/scholar?cluster=3451458773692631815[Seo Jin Park and John Ousterhout. 2019. Exploiting Commutativity For Practical Fast Replication. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19), USENIX Association, Boston, MA, 47-64. Retrieved from https://www.usenix.org/conference/nsdi19/presentation/park]
* [[[VerticalPaxosII,Vertical Paxos II]]] https://scholar.google.com/scholar?cluster=12255443511267289537[Leslie Lamport, Dahlia Malkhi, and Lidong Zhou. 2009. Vertical Paxos and Primary-Backup Replication. Microsoft. Retrieved from https://www.microsoft.com/en-us/research/publication/vertical-paxos-and-primary-backup-replication/]
* [[[CRAQ]]] https://scholar.google.com/scholar?cluster=9297968548710093419[Jeff Terrace and Michael J. Freedman. 2009. Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads. In 2009 USENIX Annual Technical Conference (USENIX ATC 09), USENIX Association, San Diego, CA. Retrieved from https://www.usenix.org/conference/usenix-09/object-storage-craq-high-throughput-chain-replication-read-mostly-workloads]
* [[[PacificA]]] https://scholar.google.com/scholar?cluster=15826444170581946812[Wei Lin, Mao Yang, Lintao Zhang, and Lidong Zhou. 2008. PacificA: Replication in Log-Based Distributed Storage Systems. Retrieved from https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/]
* [[[Hermes]]] https://scholar.google.com/scholar?cluster=13608264111814513293[Antonios Katsarakis, Vasilis Gavrielatos, M.R. Siavash Katebzadeh, Arpit Joshi, Aleksandar Dragojevic, Boris Grot, and Vijay Nagarajan. 2020. Hermes: A Fast, Fault-Tolerant and Linearizable Replication Protocol. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '20), Association for Computing Machinery, Lausanne, Switzerland, 201-217. DOI:https://doi.org/10.1145/3373376.3378496]
* [[[HyperDex]]] https://scholar.google.com/scholar?cluster=8838739194584316753[Robert Escriva, Bernard Wong, and Emin Gün Sirer. 2012. HyperDex: a distributed, searchable key-value store. In Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM '12), Association for Computing Machinery, Helsinki, Finland, 25-36. DOI:https://doi.org/10.1145/2342356.2342360]
* [[[Kafka]]] https://scholar.google.com/scholar?cluster=5891925114546481347[Jay Kreps, Neha Narkhede, Jun Rao, and others. 2011. Kafka: A distributed messaging system for log processing. In Proceedings of the NetDB, Athens, Greece, 1-7.]
* [[[FoundationDB]]] https://scholar.google.com/scholar?cluster=4197497039785350505[Jingyu Zhou, Meng Xu, Alexander Shraer, Bala Namasivayam, Alex Miller, Evan Tschannen, Steve Atherton, Andrew J. Beamon, Rusty Sears, John Leach, Dave Rosenthal, Xin Dong, Will Wilson, Ben Collins, David Scherer, Alec Grieser, Young Liu, Alvin Moore, Bhaskar Muppana, Xiaoge Su, and Vishesh Yadav. 2021. FoundationDB: A Distributed Unbundled Transactional Key Value Store. In Proceedings of the 2021 International Conference on Management of Data (SIGMOD '21), Association for Computing Machinery, Virtual Event, China, 2653-2666. DOI:https://doi.org/10.1145/3448016.3457559]
* [[[RSPaxos,RS-Paxos]]] https://scholar.google.com/scholar?cluster=16520033292975033789[Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing (HPDC '14), Association for Computing Machinery, Vancouver, BC, Canada, 61-72. DOI:https://doi.org/10.1145/2600212.2600218]
* [[[GrayFailureAchillesHeel,Gray Failure: The Achilles’ Heel of Cloud-Scale Systems]]] https://scholar.google.com/scholar?cluster=4369373863260707505[Peng Huang, Chuanxiong Guo, Lidong Zhou, Jacob R. Lorch, Yingnong Dang, Murali Chintalapati, and Randolph Yao. 2017. Gray Failure: The Achilles’ Heel of Cloud-Scale Systems. In Proceedings of the 16th Workshop on Hot Topics in Operating Systems (HotOS '17), Association for Computing Machinery, Whistler, BC, Canada, 150-155. DOI:https://doi.org/10.1145/3102980.3103005]
* [[[ScaleableButWasteful,Scaleable But Wasteful]]] https://scholar.google.com/scholar?cluster=16327886782851538912[Venkata Swaroop Matte, Aleksey Charapko, and Abutalib Aghayev. 2021. Scalable but wasteful: Current state of replication in the cloud. In Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems, 42-49.]
* [[[EPaxos,Egalitarian Paxos]]] https://scholar.google.com/scholar?cluster=13655117037814714535[Iulian Moraru, David G. Andersen, and Michael Kaminsky. 2013. There is more consensus in Egalitarian parliaments. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13), Association for Computing Machinery, Farminton, Pennsylvania, 358-372. DOI:https://doi.org/10.1145/2517349.2517350]

link:2024-resource-efficency-in-replication.bib[References as BibTex]
