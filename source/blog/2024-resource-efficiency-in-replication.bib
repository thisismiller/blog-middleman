@INPROCEEDINGS{740474,
  author={Malkhi, D. and Reiter, M.K.},
  booktitle={Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281)}, 
  title={Secure and scalable replication in Phalanx}, 
  year={1998},
  volume={},
  number={},
  pages={51-58},
  keywords={Public key;Voting;Identity-based encryption;Publishing;Reactive power;Application software;Software systems;Buildings;Large-scale systems;Online services},
  doi={10.1109/RELDIS.1998.740474}
}
@article{10.1145/200836.200869,
    author = {Attiya, Hagit and Bar-Noy, Amotz and Dolev, Danny},
    title = {Sharing memory robustly in message-passing systems},
    year = {1995},
    issue_date = {Jan. 1995},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {42},
    number = {1},
    issn = {0004-5411},
    url = {https://doi.org/10.1145/200836.200869},
    doi = {10.1145/200836.200869},
    abstract = {Emulators that translate algorithms from the shared-memory model to two different message-passing models are presented. Both are achieved by implementing a wait-free, atomic, single-writer multi-reader register in unreliable, asynchronous networks. The two message-passing models considered are a complete network with processor failures and an arbitrary network with dynamic link failures.These results make it possible to view the shared-memory model as a higher-level language for designing algorithms in asynchronous distributed systems. Any wait-free algorithm based on atomic, single-writer multi-reader registers can be automatically emulated in message-passing systems, provided that at least a majority of the processors are not faulty and remain connected. The overhead introduced by these emulations is polynomial in the number of processors in the system.Immediate new results are obtained by applying the emulators to known shared-memory algorithms. These include, among others, protocols to solve the following problems in the message-passing model in the presence of processor or link failures: multi-writer multi-reader registers, concurrent time-stamp systems, l-exclusion, atomic snapshots, randomized consensus, and implementation of data structures.},
    journal = {J. ACM},
    month = {jan},
    pages = {124-142},
    numpages = {19},
    keywords = {atomic registers, emulation, fault-tolerance, message passing, processor and link failures, shared memory, wait-freedom}
}
@inproceedings{36971,
    title	= {Megastore: Providing Scalable, Highly Available Storage for Interactive Services},author	= {Jason Baker and Chris Bond and James C. Corbett and JJ Furman and Andrey Khorlin and James Larson and Jean-Michel Leon and Yawei Li and Alexander Lloyd and Vadim Yushprakh},
    year	= {2011},
    URL	= {http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf},booktitle	= {Proceedings of the Conference on Innovative Data system Research (CIDR)},
    pages	= {223--234}
}
@article{10.14778/3137765.3137778,
    author = {Zheng, Jianjun and Lin, Qian and Xu, Jiatao and Wei, Cheng and Zeng, Chuwei and Yang, Pingan and Zhang, Yunfan},
    title = {PaxosStore: high-availability storage made practical in WeChat},
    year = {2017},
    issue_date = {August 2017},
    publisher = {VLDB Endowment},
    volume = {10},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3137765.3137778},
    doi = {10.14778/3137765.3137778},
    abstract = {In this paper, we present PaxosStore, a high-availability storage system developed to support the comprehensive business of WeChat. It employs a combinational design in the storage layer to engage multiple storage engines constructed for different storage models. PaxosStore is characteristic of extracting the Paxos-based distributed consensus protocol as a middleware that is universally accessible to the underlying multi-model storage engines. This facilitates tuning, maintaining, scaling and extending the storage engines. According to our experience in engineering practice, to achieve a practical consistent read/write protocol is far more complex than its theory. To tackle such engineering complexity, we propose a layered design of the Paxos-based storage protocol stack, where PaxosLog, the key data structure used in the protocol, is devised to bridge the programming-oriented consistent read/write to the storage-oriented Paxos procedure. Additionally, we present optimizations based on Paxos that made fault-tolerance more efficient. Discussion throughout the paper primarily focuses on pragmatic solutions that could be insightful for building practical distributed storage systems.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {1730–1741},
    numpages = {12}
}
@article{10.1145/1773912.1773922,
    author = {Lakshman, Avinash and Malik, Prashant},
    title = {Cassandra: a decentralized structured storage system},
    year = {2010},
    issue_date = {April 2010},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {44},
    number = {2},
    issn = {0163-5980},
    url = {https://doi.org/10.1145/1773912.1773922},
    doi = {10.1145/1773912.1773922},
    abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {apr},
    pages = {35–40},
    numpages = {6}
}
@inproceedings{10.5555/2643634.2643666,
    author = {Ongaro, Diego and Ousterhout, John},
    title = {In search of an understandable consensus algorithm},
    year = {2014},
    isbn = {9781931971102},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
    booktitle = {Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference},
    pages = {305–320},
    numpages = {16},
    location = {Philadelphia, PA},
    series = {USENIX ATC'14}
}
@article{10.1145/2673577,
    author = {Van Renesse, Robbert and Altinbuken, Deniz},
    title = {Paxos Made Moderately Complex},
    year = {2015},
    issue_date = {April 2015},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {47},
    number = {3},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/2673577},
    doi = {10.1145/2673577},
    abstract = {This article explains the full reconfigurable multidecree Paxos (or multi-Paxos) protocol. Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. We provide pseudocode and explain it guided by invariants. We initially avoid optimizations that complicate comprehension. Next we discuss liveness, list various optimizations that make the protocol practical, and present variants of the protocol.},
    journal = {ACM Comput. Surv.},
    month = feb,
    articleno = {42},
    numpages = {36},
    keywords = {consensus, voting, Replicated state machines}
}
@inproceedings{10.1109/DSN.2011.5958223,
    author = {Junqueira, Flavio P. and Reed, Benjamin C. and Serafini, Marco},
    title = {Zab: High-performance broadcast for primary-backup systems},
    year = {2011},
    isbn = {9781424492329},
    publisher = {IEEE Computer Society},
    address = {USA},
    url = {https://doi.org/10.1109/DSN.2011.5958223},
    doi = {10.1109/DSN.2011.5958223},
    abstract = {Zab is a crash-recovery atomic broadcast algorithm we designed for the ZooKeeper coordination service. ZooKeeper implements a primary-backup scheme in which a primary process executes clients operations and uses Zab to propagate the corresponding incremental state changes to backup processes1. Due the dependence of an incremental state change on the sequence of changes previously generated, Zab must guarantee that if it delivers a given state change, then all other changes it depends upon must be delivered first. Since primaries may crash, Zab must satisfy this requirement despite crashes of primaries.},
    booktitle = {Proceedings of the 2011 IEEE/IFIP 41st International Conference on Dependable Systems&Networks},
    pages = {245–256},
    numpages = {12},
    series = {DSN '11}
}
@inproceedings{10.1145/3318464.3386134,
    author = {Taft, Rebecca and Sharif, Irfan and Matei, Andrei and VanBenschoten, Nathan and Lewis, Jordan and Grieger, Tobias and Niemi, Kai and Woods, Andy and Birzin, Anne and Poss, Raphael and Bardea, Paul and Ranade, Amruta and Darnell, Ben and Gruneir, Bram and Jaffray, Justin and Zhang, Lucy and Mattis, Peter},
    title = {CockroachDB: The Resilient Geo-Distributed SQL Database},
    year = {2020},
    isbn = {9781450367356},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3318464.3386134},
    doi = {10.1145/3318464.3386134},
    abstract = {We live in an increasingly interconnected world, with many organizations operating across countries or even continents. To serve their global user base, organizations are replacing their legacy DBMSs with cloud-based systems capable of scaling OLTP workloads to millions of users. CockroachDB is a scalable SQL DBMS that was built from the ground up to support these global OLTP workloads while maintaining high availability and strong consistency. Just like its namesake, CockroachDB is resilient to disasters through replication and automatic recovery mechanisms. This paper presents the design of CockroachDB and its novel transaction model that supports consistent geo-distributed transactions on commodity hardware. We describe how CockroachDB replicates and distributes data to achieve fault tolerance and high performance, as well as how its distributed SQL layer automatically scales with the size of the database cluster while providing the standard SQL interface that users expect. Finally, we present a comprehensive performance evaluation and share a couple of case studies of CockroachDB users. We conclude by describing lessons learned while building CockroachDB over the last five years.},
    booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
    pages = {1493–1509},
    numpages = {17},
    location = {Portland, OR, USA},
    series = {SIGMOD '20}
}
@article{10.14778/3415478.3415535,
    author = {Huang, Dongxu and Liu, Qi and Cui, Qiu and Fang, Zhuhe and Ma, Xiaoyu and Xu, Fei and Shen, Li and Tang, Liu and Zhou, Yuxing and Huang, Menglong and Wei, Wan and Liu, Cong and Zhang, Jian and Li, Jianjun and Wu, Xuelian and Song, Lingyu and Sun, Ruoxi and Yu, Shuaipeng and Zhao, Lei and Cameron, Nicholas and Pei, Liquan and Tang, Xin},
    title = {TiDB: a Raft-based HTAP database},
    year = {2020},
    issue_date = {August 2020},
    publisher = {VLDB Endowment},
    volume = {13},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3415478.3415535},
    doi = {10.14778/3415478.3415535},
    abstract = {Hybrid Transactional and Analytical Processing (HTAP) databases require processing transactional and analytical queries in isolation to remove the interference between them. To achieve this, it is necessary to maintain different replicas of data specified for the two types of queries. However, it is challenging to provide a consistent view for distributed replicas within a storage system, where analytical requests can efficiently read consistent and fresh data from transactional workloads at scale and with high availability.To meet this challenge, we propose extending replicated state machine-based consensus algorithms to provide consistent replicas for HTAP workloads. Based on this novel idea, we present a Raft-based HTAP database: TiDB. In the database, we design a multi-Raft storage system which consists of a row store and a column store. The row store is built based on the Raft algorithm. It is scalable to materialize updates from transactional requests with high availability. In particular, it asynchronously replicates Raft logs to learners which transform row format to column format for tuples, forming a real-time updatable column store. This column store allows analytical queries to efficiently read fresh and consistent data with strong isolation from transactions on the row store. Based on this storage system, we build an SQL engine to process large-scale distributed transactions and expensive analytical queries. The SQL engine optimally accesses row-format and column-format replicas of data. We also include a powerful analysis engine, TiSpark, to help TiDB connect to the Hadoop ecosystem. Comprehensive experiments show that TiDB achieves isolated high performance under CH-benCHmark, a benchmark focusing on HTAP workloads.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {3072–3084},
    numpages = {13}
}
@inproceedings {267345,
    author = {Patrick Hunt and Mahadev Konar and Flavio P. Junqueira and Benjamin Reed},
    title = {{ZooKeeper}: Wait-free Coordination for Internet-scale Systems},
    booktitle = {2010 USENIX Annual Technical Conference (USENIX ATC 10)},
    year = {2010},
    url = {https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems},
    publisher = {USENIX Association},
    month = jun
}
@inproceedings{33002,
    title	= {Paxos Made Live - An Engineering Perspective (2006 Invited Talk)},
    author	= {Tushar Deepak Chandra and Robert Griesemer and Joshua Redstone},year	= {2007},
    URL	= {http://dx.doi.org/10.1145/1281100.1281103},
    booktitle	= {Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing}
}
@inproceedings{39966,
    title	= {Spanner: Google's Globally-Distributed Database},author	= {James C. Corbett and Jeffrey Dean and Michael Epstein and Andrew Fikes and Christopher Frost and JJ Furman and Sanjay Ghemawat and Andrey Gubarev and Christopher Heiser and Peter Hochschild and Wilson Hsieh and Sebastian Kanthak and Eugene Kogan and Hongyi Li and Alexander Lloyd and Sergey Melnik and David Mwaura and David Nagle and Sean Quinlan and Rajesh Rao and Lindsay Rolig and Dale Woodford and Yasushi Saito and Christopher Taylor and Michal Szymaniak and Ruth Wang},
    year	= {2012},
    booktitle	= {OSDI}
}
@article{10.1145/121133.121169,
    author = {Liskov, Barbara and Ghemawat, Sanjay and Gruber, Robert and Johnson, Paul and Shrira, Liuba and Williams, Michael},
    title = {Replication in the harp file system},
    year = {1991},
    issue_date = {Oct. 1991},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {25},
    number = {5},
    issn = {0163-5980},
    url = {https://doi.org/10.1145/121133.121169},
    doi = {10.1145/121133.121169},
    abstract = {This paper describes the design and implementation of the Harp file system. Harp is a replicated Unix file system accessible via the VFS interface. It provides highly available and reliable storage for files and guarantees that file operations are executed atomically in spite of concurrency and failures. It uses a novel variation of the primary copy replication technique that provides good performance because it allows us to trade disk accesses for network communication. Harp is intended to be used within a file service in a distributed network; in our current implementation, it is accessed via NFS. Preliminary performance results indicate that Harp provides equal or better response time and system capacity than an unreplicated implementation of NFS that uses Unix files directly.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {sep},
    pages = {226–238},
    numpages = {13}
}
@inproceedings {246190,
    author = {Zizhong Wang and Tongliang Li and Haixia Wang and Airan Shao and Yunren Bai and Shangming Cai and Zihan Xu and Dongsheng Wang},
    title = {{CRaft}: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost},
    booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
    year = {2020},
    isbn = {978-1-939133-12-0},
    address = {Santa Clara, CA},
    pages = {297--308},
    url = {https://www.usenix.org/conference/fast20/presentation/wang-zizhong},
    publisher = {USENIX Association},
    month = feb
}
@inproceedings {234735,
    author = {Aleksey Charapko and Ailidani Ailijiang and Murat Demirbas},
    title = {Linearizable Quorum Reads in Paxos},
    booktitle = {11th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 19)},
    year = {2019},
    address = {Renton, WA},
    url = {https://www.usenix.org/conference/hotstorage19/presentation/charapko},
    publisher = {USENIX Association},
    month = jul
}
@inproceedings{10.1145/2670979.2671001,
    author = {Moraru, Iulian and Andersen, David G. and Kaminsky, Michael},
    title = {Paxos Quorum Leases: Fast Reads Without Sacrificing Writes},
    year = {2014},
    isbn = {9781450332521},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2670979.2671001},
    doi = {10.1145/2670979.2671001},
    abstract = {This paper describes quorum leases, a new technique that allows Paxos-based systems to perform reads with high throughput and low latency. Quorum leases do not sacrifice consistency and have only a small impact on system availability and write latency. Quorum leases allow a majority of replicas to perform strongly consistent local reads, which substantially reduces read latency at those replicas (e.g., by two orders of magnitude in wide-area scenarios). Previous techniques for performing local reads in Paxos systems either (a) sacrifice consistency; (b) allow only one replica to read locally; or (c) decrease the availability of the system and increase the latency of all updates by requiring all replicas to be notified synchronously. We describe the design of quorum leases and evaluate their benefits compared to previous approaches through an implementation running in five geo-distributed Amazon EC2 datacenters.},
    booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
    pages = {1–13},
    numpages = {13},
    location = {Seattle, WA, USA},
    series = {SOCC '14}
}
@inproceedings {265017,
    author = {Siyuan Zhou and Shuai Mu},
    title = {{Fault-Tolerant} Replication with {Pull-Based} Consensus in {MongoDB}},
    booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
    year = {2021},
    isbn = {978-1-939133-21-2},
    pages = {687--703},
    url = {https://www.usenix.org/conference/nsdi21/presentation/zhou},
    publisher = {USENIX Association},
    month = apr
}
@inproceedings{10.1145/3448016.3452834,
    author = {Charapko, Aleksey and Ailijiang, Ailidani and Demirbas, Murat},
    title = {PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus},
    year = {2021},
    isbn = {9781450383431},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3448016.3452834},
    doi = {10.1145/3448016.3452834},
    abstract = {Strongly consistent replication helps keep application logic simple and provides significant benefits for correctness and manageability. Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and performance. To alleviate the leader bottleneck in strongly-consistent replication protocols, we introduce Pig, an in-protocol communication aggregation and piggybacking technique. Pig employs randomly selected nodes from follower subgroups to relay the leader's message to the rest of the followers in the subgroup, and to perform in-network aggregation of acknowledgments back from these followers. By randomly alternating the relay nodes across replication operations, Pig shields the relay nodes as well as the leader from becoming hotspots and improves throughput scalability. We showcase Pig in the context of classical Paxos protocols employed for strongly consistent replication by many cloud computing services and databases. We implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols under various workloads over clusters of size 5 to 25 nodes. We show that the aggregation at the relay has little latency overhead, and PigPaxos can provide more than 3 folds improved throughput over Paxos and EPaxos with little latency deterioration. We support our experimental observations with the analytical modeling of the bottlenecks and show that the communication bottlenecks are minimized when employing only one randomly rotating relay node.},
    booktitle = {Proceedings of the 2021 International Conference on Management of Data},
    pages = {235–247},
    numpages = {13},
    keywords = {distributed consensus, linearizability, paxos, replication},
    location = {Virtual Event, China},
    series = {SIGMOD '21}
}